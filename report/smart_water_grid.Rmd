---
title: "Correlating water quality fluctuations with sensor data"
subtitle: "Vitens Open Data Challenge: Smart Water Grid"
author: "Dennis van den Berg"
date: "24/01/2016"
output:
  html_document:
    fig_width: 10
---


## Introduction

For the Vitens Open Data Challenge we analysed sensor data measured by the Smart Water Grid sensors that Vitens employs to monitor its network of drinking water pipes and conduits. Its central question: What are the correlations between changes in water quality measued by Eventlab sensors and other real-time measurements, statuses and alarm values? As a first step in our analysis we present the results of our exploratory analysis in this report.

Additional documentation can be viewed here: http://bit.ly/1PBn7h2



## Data Description

```{r, warning=FALSE, message=FALSE, message=FALSE, echo=FALSE}
library(dplyr)
library(reshape2)
library(lubridate)
library(ggplot2)
library(caret)
library(Rmisc)
library(gplots)     # for heatmap.2

dir.data <- "~/git/smart_water_grid/data"
dir.cache <- "~/git/smart_water_grid/cache"
setwd(dir.data)

createvariablefromfilename <- function(filename) {
    # Remove Dump/meetwaarde/csv parts of filename and convert to valid variable name
    make.names(gsub("(^Dump_|.csv$)", "", gsub("(.|-|_)(m|M)eetwaarde", "", filename)))
}

readfile <- function(file) {
    variablename <- createvariablefromfilename(file)
    
    data <- read.csv(file, comment.char = "#", col.names = c("Time", "Value"), colClasses = c("character", "numeric"),
                     na.strings = c("", " ", "CalcFailed", "Calc Failed", "Bad", "BadInput", "Bad Input", "PtCreated", "Pt Created", "CommFail", "ScanOff", "Configure", "I/OTimeout"))
    
    # Check for empty file
    if(nrow(data) != 0) {
        # Convert to POSIXct/POSIXt time format
        data$Time <- ymd_hms(data$Time, tz = "CET")
        
        # Add variablename as column
        data$Variable <- variablename
    } else {
        # Add Variable column
        data <- data.frame(Time=character(), Value=numeric(), Variable=character())
    }

    # Reorder columns
    return(data[, c(1, 3, 2)])
}

readfilesraw <- function(fileslist) {
    listofdataframes <- lapply(fileslist, function(file) readfile(file))
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

# Read files and aggregate by time
readfilesandaggregate <- function(fileslist, aggregate.period="hours") {
    listofdataframes <- lapply(fileslist, function(file) aggregatebytime(readfile(file), period=aggregate.period))
    
    # Merge vertically
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

aggregatebytime <- function(dataframe, period="hours") {
    allowedperiods <- c("none", "secs", "mins", "hours", "days", "weeks", "months", "quarters", "years")

    # Calculate new aggregate vars 'mean', 'var', 'min', 'max'
    statistics <- function(values) { c(mean=mean(values), var=var(values), min=min(values), max=max(values)) }

    # Checks
    if (nrow(na.omit(dataframe)) == 0) {
        return(na.omit(dataframe))
    }
     if (! period %in% allowedperiods) {
         warning(paste("Invalid period option ", "'", period, "'", sep=""))         
     } else {
         if (! period == "none") {
             # Break into time periods
             dataframe$Time <- as.POSIXct(cut(dataframe$Time, breaks=period))
         }
     }
    
    # Aggregate and convert to proper data frame
    newdataframe <- as.data.frame(as.list(aggregate(Value ~ Time + Variable, dataframe, FUN=statistics)))
    
    # Give proper variable name
    names(newdataframe) <- gsub("Value.", "", names(newdataframe))
    
    return(newdataframe)
}

selecttimerange <- function(dataframe, begintime = -Inf, endtime = Inf) {
    subset(dataframe, Time >= begintime & Time <= endtime)
}

wideformat <- function(df.timevariablevalue) {
    # Melt 'Statistics' column
    df.responsepredictors.molten <- melt(df.timevariablevalue, id.vars=c("Time", "Variable"), variable.name="Statistic", value.name="Value")
    
    # Cast
    dcast(df.responsepredictors.molten, Time ~ Variable + Statistic, value.var="Value")
}

drawheatmap <- function(dataframe) {
    dataframe.wide <- wideformat(dataframe)
    matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
    rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
    heatmap.2(matrixwithouttimecolumn, Rowv=NA, Colv=NA, na.color='Grey', scale="column", trace="none", margins=c(5,5))
}

drawheatmapwithdendrogram <- function(dataframe) {
    dataframe.wide <- wideformat(dataframe)
    matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
    rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
    heatmap.2(matrixwithouttimecolumn, Rowv=NA, Colv=TRUE, na.color='Grey', scale="column", trace="none", margins=c(5,5))
}
```


### Data files

```{r, echo=FALSE}
files.all <- list.files(path=dir.data, pattern="csv$")
```

Our data set contains `r length(files.all)` files and its total size is a little over 5 GB. All files are in .csv format and contain Timestamp-Value pairs. Furthermore the data files contain some additional information in the header comments. 

The file name contains the variable that was measured, in format `Dump_<variable>.csv`:

```{r, echo=FALSE}
head(files.all, n=4)
```



### Time range

According to the documentation measurements are done in the time range January 2014 to December 2015. We confirmed this by looking at the data. We also observe that some variables are only available in specific time ranges, often data is missing in large blocks of this 2-year time range.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
files.select <- list.files(path=dir.data, pattern="vitnor.*csv$")[1:3]
data.select <- readfilesraw(files.select)
summary(data.select)
ggplot(data.select, aes(x=Time, y=Value, color=Variable)) + geom_point() + ggtitle("Raw data (unaggregated)")
```

The frequency of data logging is 1 measurement per minute at most, though certainly not for all variables.


### Measured Quantities

In the documentation we see the following explanation of variable names:

Quantity      | Variable code    | Remarks
------------- | -------------    | -------------
Eventlab data | `*vitnor*`       | 3 per sensor (`vitnor1`, `vitnor2`, `vitnor3`). Alarm codes are given in case 1 or more values above threshold (code Orange for `1 > max(vitnor*) >= 1.5`, code Red for `max(vitnor*) > 1.5`)
Temperature   | `*TM*`           | 
Flow          | `*FT*`, `*VO*`   | Negative values allowed (in case of opposite flow direction)
Pressure      | `*PT*`, `*DO*`   | We found that these had to be followed by 2 digits (`*PTxx*`, `*DOxx*`) in order not to select incorrect variables 
Conductivity  | `FR-PNB_TR00QI03PV*`, `FR-POH_-TR00QI03PV*` | 
Acidity (pH)  | `FR-PNB_TR00QI01PV*`, `FR-POH_-TR00QI02PV*`, `FR-PSP-TR00QI01*`, `FR-PTW_TR01QI01PV*` | 
Turbidity     | `FR-PTW_TR01QI02PV*`, `FR-PSP-TR00QI02*`, `FR-PNB_TR00QI02PV*`, `FR-POH_-TR00QI01PV*` | 
Other         | (None of the above) | Status values of pumps, reservoirs, valves, etc

```{r, echo=FALSE}
files.eventlab <- list.files(path=dir.data, pattern="vitnor.*csv$")
files.temperature <- list.files(path=dir.data, pattern="TM.*csv$")
files.flow <- list.files(path=dir.data, pattern="(FT|VO).*csv$")
files.pressure <- list.files(path=dir.data, pattern="(PT|DO)[0123456789][0123456789]-.*csv$")
files.conductivity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI03PV|FR-POH_-TR00QI03PV).*csv$")
files.acidity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI01PV|FR-POH_-TR00QI02PV|FR-PSP-TR00QI01|FR-PTW_TR01QI01PV).*csv$")
files.turbidity <- list.files(path=dir.data, pattern="(FR-PTW_TR01QI02PV|FR-PSP-TR00QI02|FR-PNB_TR00QI02PV|FR-POH_-TR00QI01PV).*csv$")
files.other <- Reduce(setdiff, list(files.all, files.eventlab, files.temperature, files.flow, files.pressure, files.conductivity, files.acidity, files.turbidity))
```


### Predictor and response variables

We are interested in Eventlab measurements (variable names `*vitnor*`) as **response variables**

* There are `r length(files.eventlab)` variables for Eventlab measurements
* These represent `r length(files.eventlab)/3` Eventlab sensors, each of which produce a set of three variables (`*vitnor1*`, `*vitnor2*` and `*vitnor3*`)
* Specifically, we are interested in events with values above a threshold for these variables (>1 or >1.5) or with high fluctuations in a short time frame (variance).

That leaves `r length(files.all)-length(files.eventlab)` variables that we can use as **potential predictors**:

* `r length(files.temperature)` for temperature
* `r length(files.flow)` for flow
* `r length(files.pressure)` for pressure
* `r length(files.conductivity)` for conductivity
* `r length(files.acidity)` for acidity
* `r length(files.turbidity)` for turbidity
* `r length(files.other)` other

The data quality of these variables varies. Some files do not even contain any data points.


### Zooming in on raw Eventlab data

```{r, echo=FALSE}
pattern.response="FR-MOBMS-vitnor1"
time.begin.1=ymd("2015-06-27", tz = "CET")
time.end.1=ymd("2015-07-04", tz = "CET")
time.begin.2=ymd_hms("2015-06-29T00:00:00", tz = "CET")
time.end.2=ymd_hms("2015-06-29T12:00:00", tz = "CET")
time.begin.3=ymd_hms("2015-06-29T02:00:00", tz = "CET")
time.end.3=ymd_hms("2015-06-29T03:00:00", tz = "CET")
threshold.orange=1
threshold.red=1.5
files.response <- list.files(path=dir.data, pattern=pattern.response)
variables.response <- createvariablefromfilename(files.response)
```

Based on large blocks of consecutive data seen in exploratory plots and multiple peaks of Eventlab variables exceeding the threshold we decided to focus on the `r variables.response` variable as response variable.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
df.eventlab.response <- readfilesraw(files.response)
plot.response <- ggplot(df.eventlab.response, aes(x=Time, y=Value)) + geom_point() + geom_hline(yintercept=threshold.orange, color="orange") + geom_hline(yintercept=threshold.red, color="red") + guides(col=guide_legend(ncol=1))
plot.response + ggtitle("Eventlab - days timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.1, time.end.1)))
plot.response + ggtitle("Eventlab - hours timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.2, time.end.2)))
```
Zooming in on a specific Eventlab alarm event starting around `r time.begin.3` we observe that the Eventlab values can rise and exceed the threshold (>1 resp >1.5) abruptly in a matter of minutes and that this process can repeat itself in less than an hour:
```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.response + ggtitle("Eventlab - minutes timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.3, time.end.3)))
```



## Challenges and approach

Some challenges we encountered:

* Measuring frequency differs per data set, blocks of missing data, string values in numeric variables
* Lot of data, consisting of many response variables and many potential predictors
* Potential time lags in correlation

Start of our approach was to do the following:

1. Aggregating hourly statistics: looking at mean, variance, min and max per hour
2. Subsetting: by selecting variables and time range

We decided to aggregate data for multiple reasons: in order to compress our data set, to increase the chances of different variables both having a value for a given time, to aggregate the number of peak events (in Eventlab measurements, not one event per minute) and also to increase our chances of finding correlated signals in which a small time lag between variables is present (~ hour time scale).

With regard to subsetting, we selected temperature, flow, pressure, conductivity, acidity and turbidity as potential predictor variables and Eventlab measurements as response variables. We chose not to look into the 'other' measured quantities for now, especially since we have limited background of their meaning. Also, we looked at specific time ranges in order to simplify our investigation.

We then try to find relationships between measured quantities by looking at correlations of their aggregated statistics. For example: Since we are interested in changes (deltas), one approach is to correlate Eventlab hourly variance with hourly variance in flow measurements.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
# df.eventlab.hourly <- readfilesandaggregate(files.eventlab, aggregate.period="hours")
# df.temperature.hourly <- readfilesandaggregate(files.temperature, aggregate.period="hours")
# df.flow.hourly <- readfilesandaggregate(files.flow, aggregate.period="hours")
# df.pressure.hourly <- readfilesandaggregate(files.pressure, aggregate.period="hours")
# df.conductivity.hourly <- readfilesandaggregate(files.conductivity, aggregate.period="hours")
# df.acidity.hourly <- readfilesandaggregate(files.acidity, aggregate.period="hours")
# df.turbidity.hourly <- readfilesandaggregate(files.turbidity, aggregate.period="hours")
# # Ignoring 'other' variables for now

# # Write/read aggregated datasets to/from dir.cache
setwd(dir.cache)
# save(df.eventlab.hourly, file="eventlab.hourly.Rdata")
# save(df.temperature.hourly, file="temperature.hourly.Rdata")
# save(df.flow.hourly, file="flow.hourly.Rdata")
# save(df.pressure.hourly, file="pressure.hourly.Rdata")
# save(df.conductivity.hourly, file="conductivity.hourly.Rdata")
# save(df.acidity.hourly, file="acidity.hourly.Rdata")
# save(df.turbidity.hourly, file="turbidity.hourly.Rdata")
load(file="eventlab.hourly.Rdata")
load(file="temperature.hourly.Rdata")
load(file="flow.hourly.Rdata")
load(file="pressure.hourly.Rdata")
load(file="conductivity.hourly.Rdata")
load(file="acidity.hourly.Rdata")
load(file="turbidity.hourly.Rdata")
setwd(dir.data)
```



## Exploratory Analysis

### Eventlab hourly maximums and variances

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.max <- ggplot(df.eventlab.hourly, aes(x=Time, y=max, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly max)") + geom_hline(yintercept=1, color="orange") + geom_hline(yintercept=1.5, color="red") + guides(col=guide_legend(ncol=1))
plot.eventlab.hourly.max + scale_y_log10()
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.var <- ggplot(df.eventlab.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.eventlab.hourly.var + scale_y_log10()
```

We notice many peak events above the orange and red thresholds (>1 resp >1.5) for Eventlab sensor data.


### Hourly means for (potential) predictor variables

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.mean <- ggplot(df.temperature.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Temperature (hourly means)") + guides(col=guide_legend(ncol=1))
plot.flow.hourly.mean <- ggplot(df.flow.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Flow (hourly means)") + guides(col=guide_legend(ncol=1))
plot.pressure.hourly.mean <- ggplot(df.pressure.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Pressure (hourly means)") + guides(col=guide_legend(ncol=1))
plot.conductivity.hourly.mean <- ggplot(df.conductivity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly means)") + guides(col=guide_legend(ncol=1))
plot.acidity.hourly.mean <- ggplot(df.acidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Acidity (hourly means)") + guides(col=guide_legend(ncol=1))
plot.turbidity.hourly.mean <- ggplot(df.turbidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly means)") + guides(col=guide_legend(ncol=1))

plot.temperature.hourly.mean
plot.flow.hourly.mean
plot.pressure.hourly.mean + scale_y_log10()
plot.conductivity.hourly.mean + scale_y_log10()
plot.acidity.hourly.mean + scale_y_log10()
plot.turbidity.hourly.mean + scale_y_log10()
```


### Hourly variances

Because we are interested in correlated peak events, we also looked at the hourly variance of these variables, as a measure of their variability on the timescale of less than an hour.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.var <- ggplot(df.temperature.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Temperature (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.flow.hourly.var <- ggplot(df.flow.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Flow (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.pressure.hourly.var <- ggplot(df.pressure.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Pressure (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.conductivity.hourly.var <- ggplot(df.conductivity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.acidity.hourly.var <- ggplot(df.acidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Acidity (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.turbidity.hourly.var <- ggplot(df.turbidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly variance)") + guides(col=guide_legend(ncol=1))

plot.temperature.hourly.var + scale_y_log10()
plot.flow.hourly.var + scale_y_log10()
plot.pressure.hourly.var + scale_y_log10()
plot.conductivity.hourly.var + scale_y_log10()
plot.acidity.hourly.var + scale_y_log10()
plot.turbidity.hourly.var + scale_y_log10()
```



### Heatmaps of Eventlab alarm events


Relation between different Eventlab alarm events (within certain time period)

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}

selectedvariables <- levels(df.eventlab.hourly$Variable)
df.eventlab.selection <- subset(df.eventlab.hourly,
                                Variable %in% selectedvariables &
                                Time >= ymd_hms("2015-06-01T00:00:00", tz = "CET") & Time <= ymd_hms("2015-07-04T12:00:00", tz = "CET"),
                                select=c(Time, Variable, max)
)

# Calculate alarm if max > 1
df.eventlab.selection$alarm <- 1 * (df.eventlab.selection$max > 1)
df.eventlab.selection <- subset(df.eventlab.selection, select=-max)

#drawheatmap(df.eventlab.selection)
drawheatmapwithdendrogram(df.eventlab.selection)
# Might give us ideas about which Eventlab sensor alarms are related
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
# Select subset of vitnor variables, read data for specific time range and plot heatmap
setwd(dir.data)
pattern.eventlab="FR-MOBMS-vitnor"
files.eventlab <- list.files(path=dir.data, pattern=pattern.eventlab)
data.eventlab <- readfilesraw(files.eventlab)
time.begin=ymd_hms("2015-05-20T00:00:00", tz = "CET")
time.end=ymd_hms("2015-06-30T00:00:00", tz = "CET")
data.eventlab.timerange <- selecttimerange(data.eventlab, time.begin, time.end)
#heatmap.abovethreshold(data.eventlab.timerange, threshold=1)
```


### Correlations


## Preliminary Conclusions

* We gained insight into measurement ranges and variability of Eventlab data, temperature, pressure, conductivity, acidity and turbidity data
* Eventlab values can rise and exceed the threshold (>1 resp >1.5) abruptly in a matter of minutes and that this process can repeat itself in less than an hour

* Possible correlations ..


## Further Research

#### Additional methods

1. **Clustering** of variables (exploratory)
    * Mostly useful to investigate which variables are closely related
    * But works best *within* datasets of similar measured quantities (such as *only* flow data or pressure data)
1. **Regression Analysis**
    * Complicated because of lack of domain knowledge (risk of including theoretically unrelated regressors, etc)
1. **Machine Learning**:
    * Specify *categorization problem* in which the Eventlab variable has 2 status levels 'normal'/'alarm' if below/above threshold (or even better: 'green'/'orange'/'red')
    * Split dataset into training and test sets
    * Apply ML algorithm (Random Forest, Boosting?)
    * Predict and validate
    * We are intested in list of most important predictors!


#### Improved subsetting and aggregation

* **Extend time period**: larger data subsets to train more accurate models
* **Different response variables**: instead of the specific one we focused on right now
* **More measured quantities**: for example include 'other' variables (more domain knowledge needed?)
* **Smaller/larger aggregation time windows**: or don't aggregate at all
* **Aggregate different statistics**: now we determined mean, variance, minimum, maximum, anyabovethreshold per aggregation time window
* **Correlate different statistics combinations**: for example look at correlation of hourly max for Eventlab with hourly min of pressure)


#### Incorporate domain knowledge

Last but not least: it's safe to say that including more domain knowledge about **measured quantities** and **how they were gathered** may make it substantially easier to focus on more effective approaches and to draw meaningful conclusions.



## Data Source

Data set provided by Vitens.