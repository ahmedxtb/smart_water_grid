---
title: "Correlating water quality fluctuations with sensor data"
subtitle: "Vitens Open Data Challenge: Smart Water Grid"
author: "Dennis van den Berg"
date: "24/01/2016"
output:
  html_document:
    fig_width: 10
---


## Introduction

For the Vitens Open Data Challenge we analysed sensor data measured by the Smart Water Grid sensors that Vitens employs to monitor its network of drinking water pipes and conduits. Its central question: What are the correlations between changes in water quality measued by Eventlab sensors and other real-time measurements, statuses and alarm values? As a first step in our analysis we present the results of our exploratory analysis in this report.

Our most important result so far is that the variance of one of the turbidity variables (FR-PTW_TR01QI02PV) correlates strongly (negatively) and significantly (p<0.05) with the variance in two of the Eventlab variables (FR-MOALD-vitnor1 and FR-MOALD-vitnor3).

Additional documentation can be viewed here: http://bit.ly/1PBn7h2



## Data Description

```{r, warning=FALSE, message=FALSE, message=FALSE, echo=FALSE}
library(dplyr, quiet=TRUE)
library(reshape2, quiet=TRUE)
library(lubridate, quiet=TRUE)
library(ggplot2, quiet=TRUE)
library(caret, quiet=TRUE)
library(Rmisc, quiet=TRUE)
library(gplots, quiet=TRUE)       # for heatmap.2
library(corrplot, quiet=TRUE)
library(Hmisc, quiet=TRUE)        # for rcorr

dir.data <- "~/git/smart_water_grid/data"
dir.cache <- "~/git/smart_water_grid/cache"
setwd(dir.data)

createvariablefromfilename <- function(filename) {
    # Remove Dump/meetwaarde/csv parts of filename and convert to valid variable name
    make.names(gsub("(^Dump_|.csv$)", "", gsub("(.|-|_)(m|M)eetwaarde", "", filename)))
}

readfile <- function(file) {
    variablename <- createvariablefromfilename(file)
    
    data <- read.csv(file, comment.char = "#", col.names = c("Time", "Value"), colClasses = c("character", "numeric"),
                     na.strings = c("", " ", "CalcFailed", "Calc Failed", "Bad", "BadInput", "Bad Input", "PtCreated", "Pt Created", "CommFail", "ScanOff", "Configure", "I/OTimeout"))
    
    # Check for empty file
    if(nrow(data) != 0) {
        # Convert to POSIXct/POSIXt time format
        data$Time <- ymd_hms(data$Time, tz = "CET")
        
        # Add variablename as column
        data$Variable <- variablename
    } else {
        # Add Variable column
        data <- data.frame(Time=character(), Value=numeric(), Variable=character())
    }

    # Reorder columns
    return(data[, c(1, 3, 2)])
}

readfilesraw <- function(fileslist) {
    listofdataframes <- lapply(fileslist, function(file) readfile(file))
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

# Read files and aggregate by time
readfilesandaggregate <- function(fileslist, aggregate.period="hours") {
    listofdataframes <- lapply(fileslist, function(file) aggregatebytime(readfile(file), period=aggregate.period))
    
    # Merge vertically
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

aggregatebytime <- function(dataframe, period="hours") {
    allowedperiods <- c("none", "secs", "mins", "hours", "days", "weeks", "months", "quarters", "years")

    # Calculate new aggregate vars 'mean', 'var', 'min', 'max'
    statistics <- function(values) { c(mean=mean(values), var=var(values), min=min(values), max=max(values)) }

    # Checks
    if (nrow(na.omit(dataframe)) == 0) {
        return(na.omit(dataframe))
    }
     if (! period %in% allowedperiods) {
         warning(paste("Invalid period option ", "'", period, "'", sep=""))         
     } else {
         if (! period == "none") {
             # Break into time periods
             dataframe$Time <- as.POSIXct(cut(dataframe$Time, breaks=period))
         }
     }
    
    # Aggregate and convert to proper data frame
    newdataframe <- as.data.frame(as.list(aggregate(Value ~ Time + Variable, dataframe, FUN=statistics)))
    
    # Give proper variable name
    names(newdataframe) <- gsub("Value.", "", names(newdataframe))
    
    return(newdataframe)
}

selecttimerange <- function(dataframe, begintime = -Inf, endtime = Inf) {
    subset(dataframe, Time >= begintime & Time <= endtime)
}

wideformat <- function(df.timevariablevalue) {
    # Melt 'Statistics' column
    df.responsepredictors.molten <- melt(df.timevariablevalue, id.vars=c("Time", "Variable"), variable.name="Statistic", value.name="Value")
    
    # Cast
    dcast(df.responsepredictors.molten, Time ~ Variable + Statistic, value.var="Value")
}

drawheatmap <- function(dataframe) {
    dataframe.wide <- wideformat(dataframe)
    matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
    rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
    heatmap.2(matrixwithouttimecolumn, Rowv=NA, Colv=NA, na.color='Grey', scale="column", trace="none", margins=c(5,5))
}

drawheatmapwithdendrogram <- function(dataframe) {
    dataframe.wide <- wideformat(dataframe)
    matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
    rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
    heatmap.2(matrixwithouttimecolumn, Rowv=NA, Colv=TRUE, na.color='Grey', scale="column", trace="none", margins=c(5,5))
}

plotcorrelations <- function(dataframe, begintime=-Inf, endtime=Inf) {
    # Merge vertically, select time range, convert to wideformat
    dataframe.wide <- wideformat(selecttimerange(dataframe, begintime, endtime))
    
    # Drop Time column, convert to matrix and set rownames to Time column
    matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
    rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
    
    # Calculate correlation matrix
    Mcorr <- rcorr(matrixwithouttimecolumn)
    Mcorr$r[is.na(Mcorr$r)] <- 0
    
    # Plot correlations
    corrplot(Mcorr$r[], order="original", diag=FALSE, p.mat=Mcorr$P, sig.level=0.05, insig="blank")
}

# matrixwithouttime <- function(dataframe) {
#     dataframe.wide <- wideformat(dataframe)
#     matrixwithouttimecolumn <- as.matrix(subset(dataframe.wide, select=-Time))
#     rownames(matrixwithouttimecolumn) <- as.character(dataframe.wide$Time)
#     return(matrixwithouttimecolumn)
# }
```


### Data files

```{r, echo=FALSE}
files.all <- list.files(path=dir.data, pattern="csv$")
```

Our data set contains `r length(files.all)` files and its total size is a little over 5 GB. All files are in .csv format and contain Timestamp-Value pairs. Furthermore the data files contain some additional information in the header comments. 

The file name contains the variable that was measured, in format `Dump_<variable>.csv`:

```{r, echo=FALSE}
head(files.all, n=4)
```



### Time range

According to the documentation measurements are done in the time range January 2014 to December 2015. We confirmed this by looking at the data. We also observe that some variables are only available in specific time ranges, often data is missing in large blocks of this 2-year time range (see graph below).

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
files.select <- list.files(path=dir.data, pattern="vitnor.*csv$")[1:3]
data.select <- readfilesraw(files.select)
summary(data.select)
ggplot(data.select, aes(x=Time, y=Value, color=Variable)) + geom_point() + ggtitle("Raw data (unaggregated)")
```

The frequency of data logging is 1 measurement per minute at most, though certainly not for all variables.


### Measured Quantities

In the documentation we see the following explanation of variable names:

Quantity      | Variable code    | Remarks
------------- | -------------    | -------------
Eventlab data | `*vitnor*`       | 3 per sensor (`vitnor1`, `vitnor2`, `vitnor3`). Alarm codes are given in case 1 or more values above threshold (code Orange for `1 > max(vitnor*) >= 1.5`, code Red for `max(vitnor*) > 1.5`)
Temperature   | `*TM*`           | 
Flow          | `*FT*`, `*VO*`   | Negative values allowed (in case of opposite flow direction)
Pressure      | `*PT*`, `*DO*`   | We found that these had to be followed by 2 digits (`*PTxx*`, `*DOxx*`) in order not to select incorrect variables 
Conductivity  | `FR-PNB_TR00QI03PV*`, `FR-POH_-TR00QI03PV*` | 
Acidity (pH)  | `FR-PNB_TR00QI01PV*`, `FR-POH_-TR00QI02PV*`, `FR-PSP-TR00QI01*`, `FR-PTW_TR01QI01PV*` | 
Turbidity     | `FR-PTW_TR01QI02PV*`, `FR-PSP-TR00QI02*`, `FR-PNB_TR00QI02PV*`, `FR-POH_-TR00QI01PV*` | 
Other         | (None of the above) | Status values of pumps, reservoirs, valves, etc

```{r, echo=FALSE}
files.eventlab <- list.files(path=dir.data, pattern="vitnor.*csv$")
files.temperature <- list.files(path=dir.data, pattern="TM.*csv$")
files.flow <- list.files(path=dir.data, pattern="(FT|VO).*csv$")
files.pressure <- list.files(path=dir.data, pattern="(PT|DO)[0123456789][0123456789]-.*csv$")
files.conductivity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI03PV|FR-POH_-TR00QI03PV).*csv$")
files.acidity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI01PV|FR-POH_-TR00QI02PV|FR-PSP-TR00QI01|FR-PTW_TR01QI01PV).*csv$")
files.turbidity <- list.files(path=dir.data, pattern="(FR-PTW_TR01QI02PV|FR-PSP-TR00QI02|FR-PNB_TR00QI02PV|FR-POH_-TR00QI01PV).*csv$")
files.other <- Reduce(setdiff, list(files.all, files.eventlab, files.temperature, files.flow, files.pressure, files.conductivity, files.acidity, files.turbidity))
```


### Predictor and response variables

We are interested in Eventlab measurements (variable names `*vitnor*`) as **response variables**:

* There are `r length(files.eventlab)` variables for Eventlab measurements
* These represent `r length(files.eventlab)/3` Eventlab sensors, each of which produce a set of three variables (`*vitnor1*`, `*vitnor2*` and `*vitnor3*`)
* Specifically, we are interested in events with values above a threshold for these variables (>1 or >1.5) or with high fluctuations in a short time frame (variance).

That leaves `r length(files.all)-length(files.eventlab)` variables that we can use as **potential predictors**:

* `r length(files.temperature)` for temperature
* `r length(files.flow)` for flow
* `r length(files.pressure)` for pressure
* `r length(files.conductivity)` for conductivity
* `r length(files.acidity)` for acidity
* `r length(files.turbidity)` for turbidity
* `r length(files.other)` other

The data quality of these variables varies. Some files do not even contain any data points.


### Zooming in on raw Eventlab data

```{r, echo=FALSE}
pattern.response="FR-MOBMS-vitnor1"
time.begin.1=ymd("2015-06-27", tz = "CET")
time.end.1=ymd("2015-07-04", tz = "CET")
time.begin.2=ymd_hms("2015-06-29T00:00:00", tz = "CET")
time.end.2=ymd_hms("2015-06-29T12:00:00", tz = "CET")
time.begin.3=ymd_hms("2015-06-29T02:00:00", tz = "CET")
time.end.3=ymd_hms("2015-06-29T03:00:00", tz = "CET")
threshold.orange=1
threshold.red=1.5
files.response <- list.files(path=dir.data, pattern=pattern.response)
variables.response <- createvariablefromfilename(files.response)
```

Based on large blocks of consecutive data seen in exploratory plots and multiple peaks of Eventlab variables exceeding the threshold we decided to focus on the `r variables.response` variable as response variable as a first exploratory step:

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
df.eventlab.response <- readfilesraw(files.response)
plot.response <- ggplot(df.eventlab.response, aes(x=Time, y=Value)) + geom_point() + geom_hline(yintercept=threshold.orange, color="orange") + geom_hline(yintercept=threshold.red, color="red") + guides(col=guide_legend(ncol=1))
plot.response + ggtitle("Eventlab - days timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.1, time.end.1)))
plot.response + ggtitle("Eventlab - hours timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.2, time.end.2)))
plot.response + ggtitle("Eventlab - minutes timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.3, time.end.3)))
```

Zooming in on a specific Eventlab alarm event starting around `r time.begin.3` we observe that the Eventlab values can rise and exceed the threshold (>1 resp >1.5) in a matter of minutes, sometimes falling abruptly, sometimes more gradually, and that this process can repeat itself in less than an hour.



## Challenges and approach

Some challenges we encountered:

* Measuring frequency differs per data set, blocks of missing data, string values in numeric variables
* Lot of data, consisting of many response variables and many potential predictors
* Potential time lags in correlation

Start of our approach was to do the following:

1. Aggregating hourly statistics: looking at mean, variance, min and max per hour
2. Subsetting: by selecting variables and time range

We decided to aggregate data for multiple reasons: in order to compress our data set, to increase the chances of different variables both having a value for a given time so that they can be correlated/compared, to reduce the number of peak Eventlab events (instead of one event per minute) and also to increase our chances of finding correlated signals in which a small time lag between variables is present (~ hour time scale).

With regard to subsetting, we selected temperature, flow, pressure, conductivity, acidity and turbidity as potential predictor variables and Eventlab measurements as response variables. We chose not to look into the 'other' measured quantities for now, especially since we have limited background of their meaning. Also, we looked at specific time ranges in order to simplify our investigation.

We then try to find relationships between measured quantities by looking at correlations of their aggregated statistics. For example: Since we are interested in changes (deltas), one approach is to correlate Eventlab hourly variance with hourly variance in flow measurements.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
# df.eventlab.hourly <- readfilesandaggregate(files.eventlab, aggregate.period="hours")
# df.temperature.hourly <- readfilesandaggregate(files.temperature, aggregate.period="hours")
# df.flow.hourly <- readfilesandaggregate(files.flow, aggregate.period="hours")
# df.pressure.hourly <- readfilesandaggregate(files.pressure, aggregate.period="hours")
# df.conductivity.hourly <- readfilesandaggregate(files.conductivity, aggregate.period="hours")
# df.acidity.hourly <- readfilesandaggregate(files.acidity, aggregate.period="hours")
# df.turbidity.hourly <- readfilesandaggregate(files.turbidity, aggregate.period="hours")
# # Ignoring 'other' variables for now

# # Write/read aggregated datasets to/from dir.cache
setwd(dir.cache)
# save(df.eventlab.hourly, file="eventlab.hourly.Rdata")
# save(df.temperature.hourly, file="temperature.hourly.Rdata")
# save(df.flow.hourly, file="flow.hourly.Rdata")
# save(df.pressure.hourly, file="pressure.hourly.Rdata")
# save(df.conductivity.hourly, file="conductivity.hourly.Rdata")
# save(df.acidity.hourly, file="acidity.hourly.Rdata")
# save(df.turbidity.hourly, file="turbidity.hourly.Rdata")
load(file="eventlab.hourly.Rdata")
load(file="temperature.hourly.Rdata")
load(file="flow.hourly.Rdata")
load(file="pressure.hourly.Rdata")
load(file="conductivity.hourly.Rdata")
load(file="acidity.hourly.Rdata")
load(file="turbidity.hourly.Rdata")
setwd(dir.data)
```



## Exploratory Analysis

### Eventlab hourly maximums and variances

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.max <- ggplot(df.eventlab.hourly, aes(x=Time, y=max, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly max)") + geom_hline(yintercept=1, color="orange") + geom_hline(yintercept=1.5, color="red") + guides(col=guide_legend(ncol=1))
plot.eventlab.hourly.max + scale_y_log10()
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.var <- ggplot(df.eventlab.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.eventlab.hourly.var + scale_y_log10()
```

We notice many peak events above the orange and red thresholds (>1 resp >1.5) for Eventlab sensor data.

Furthermore, the graphs of hourly maximums and hourly variance look very similar because a high hourly max usually corresponds with an abrupt peak (within hour time window) meaning that the hourly variance is high as well.


### Hourly means for (potential) predictor variables

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.mean <- ggplot(df.temperature.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Temperature (hourly means)") + guides(col=guide_legend(ncol=1))
plot.flow.hourly.mean <- ggplot(df.flow.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Flow (hourly means)") + guides(col=guide_legend(ncol=1))
plot.pressure.hourly.mean <- ggplot(df.pressure.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Pressure (hourly means)") + guides(col=guide_legend(ncol=1))
plot.conductivity.hourly.mean <- ggplot(df.conductivity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly means)") + guides(col=guide_legend(ncol=1))
plot.acidity.hourly.mean <- ggplot(df.acidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Acidity (hourly means)") + guides(col=guide_legend(ncol=1))
plot.turbidity.hourly.mean <- ggplot(df.turbidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly means)") + guides(col=guide_legend(ncol=1))
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.mean
```

We observe what looks like steam conduits with temperature 100 degrees Celcius, some hot water measurements in the 20 to 75 degrees region, plus seasonal changes in mean water temperature.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.flow.hourly.mean
```

Mean flow is in the 0 to 700 m^3/hour region, with some outliers on both the positive and negative sides (could be leaks). In the graph below we also see negative outliers in mean pressure:

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.pressure.hourly.mean + scale_y_log10()
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.conductivity.hourly.mean + scale_y_log10()
```

We have 2 measured variables for conductivity. In the plot above we see large drops in one of these conductivity variables between March and May 2014.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.acidity.hourly.mean + scale_y_log10()
```

The single outlier in this unvalidated data makes the fluctuations in the above plot harder to read, although we see some clear positive and negative outliers.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.turbidity.hourly.mean + scale_y_log10()
```

We have 4 measured variables describing turbidity. We see some fluctuations in the long time range (months) and peaks/regions with higher turbidity. Turbidity could be related to water quality, although we are lacking specific domain knowledge here.



### Hourly variances

Because we are interested in correlated peak events, we also looked at the hourly variance of these variables, as a measure of their variability on the timescale of less than an hour.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.var <- ggplot(df.temperature.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Temperature (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.flow.hourly.var <- ggplot(df.flow.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Flow (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.pressure.hourly.var <- ggplot(df.pressure.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Pressure (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.conductivity.hourly.var <- ggplot(df.conductivity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.acidity.hourly.var <- ggplot(df.acidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Acidity (hourly variance)") + guides(col=guide_legend(ncol=1))
plot.turbidity.hourly.var <- ggplot(df.turbidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly variance)") + guides(col=guide_legend(ncol=1))
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.var + scale_y_log10()
```

Again, we see the seasonal differences in the hourly temperature variance.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.flow.hourly.var + scale_y_log10()
```

Some flow variables change a lot more (2 orders of magnitude) than others.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.pressure.hourly.var + scale_y_log10()
```

Again, we see some peaks in which the hourly pressure variance is more than 1 order of magnitude higher than normal.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.conductivity.hourly.var + scale_y_log10()
```

Due to the low frequency of measurements, hourly variance data is missing for a lot of the time, except for the PR.PNB_TR* measurements after November 2014.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.acidity.hourly.var + scale_y_log10()
```

For hourly variance in acidity, the number of data points varies between variable and furthermore we observe a lot of gaps in the data.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.turbidity.hourly.var + scale_y_log10()
```

For hourly variance in turbidity we observe some peaks and periods with higher variance (April to June 2015 for FR.PNB_*).


### Heatmaps of Eventlab alarm events

In order to investigate the relationship between different Eventlab alarm events within a specific time period, we plot a quick heat map:

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
selectedvariables <- levels(df.eventlab.hourly$Variable)
df.eventlab.selection <- subset(df.eventlab.hourly,
                                Variable %in% selectedvariables &
                                Time >= ymd_hms("2015-06-01T00:00:00", tz = "CET") & Time <= ymd_hms("2015-07-04T12:00:00", tz = "CET"),
                                select=c(Time, Variable, max)
)

# Calculate alarm if max > 1
df.eventlab.selection$alarm <- 1 * (df.eventlab.selection$max > 1)
df.eventlab.selection <- subset(df.eventlab.selection, select=-max)

#drawheatmap(df.eventlab.selection)
drawheatmapwithdendrogram(df.eventlab.selection)
# Might give us ideas about which Eventlab sensor alarms are related
```

We observe small groups (often triples) of more closely related variables. One triple displays a somewhat larger distance to the others, but furthermore no clear pattern is visible. The grey area on the right corresponds to Eventlab measurements that did not have an alarm value (above the threshold) at all in the given time frame.



### Correlations

```{r, warning=FALSE, message=FALSE, cache=FALSE, echo=FALSE}
# Select subsets and time ranges
begintime <- ymd("2015-06-01", tz = "CET")
endtime <- ymd("2015-06-30", tz = "CET")
df1 <- subset(df.eventlab.hourly, Variable %in% levels(df.eventlab.hourly$Variable)[1:20], select=c(Time, Variable, var))
df2 <- subset(df.flow.hourly, Variable %in% levels(df.flow.hourly$Variable)[1:40], select=c(Time, Variable, var))
df3 <- subset(df.pressure.hourly, Variable %in% levels(df.pressure.hourly$Variable)[1:40], select=c(Time, Variable, var))
df4 <- subset(df.acidity.hourly, select=c(Time, Variable, var))
df5 <- subset(df.turbidity.hourly, select=c(Time, Variable, var))
df6 <- subset(df.conductivity.hourly, select=c(Time, Variable, var))
df7 <- subset(df.temperature.hourly, Variable %in% levels(df.temperature.hourly$Variable)[1:40], select=c(Time, Variable, var))
```

#### Eventlab and flow variances

A correlation plot for Eventlab and flow variances mainly seems to display correlations *within* the flow and Eventlab datasets, not *between* them:

```{r, warning=FALSE, message=FALSE, cache=FALSE, echo=FALSE}
# Plot correlations
plotcorrelations(rbind(df1, df2), begintime, endtime)
```


#### Eventlab and pressure variances

A correlation plot of Eventlab and pressure variances displays a lot of correlations *within* the pressure and Eventlab datasets, not *between* them:

```{r, warning=FALSE, message=FALSE, cache=FALSE, echo=FALSE}
plotcorrelations(rbind(df1, df2, df3), begintime, endtime)
```


#### Eventlab, acidity, turbidity and conductivity variances

We find the following correlation plot for Eventlab, acidity, turbidity and conductivity variances:

```{r, warning=FALSE, message=FALSE, cache=FALSE, echo=FALSE}
plotcorrelations(rbind(df1, df4, df5, df6), begintime, endtime)
```

And observe that one of the turbidity variables correlates strongly with two of the Eventlab variables! Note that this is a negative correlation though, requiring further research.

```{r, warning=FALSE, message=FALSE, cache=FALSE, echo=FALSE}
#Correlate Eventlab with temperature variances
#plotcorrelations(rbind(df1, df7), begintime, endtime)
```


## Preliminary Conclusions

* We gained insight into measurement ranges and variability of Eventlab data, temperature, pressure, conductivity, acidity and turbidity data
* Eventlab values can rise and exceed the threshold (>1 resp >1.5) abruptly in a matter of minutes and that this process can repeat itself in less than an hour
* We observe that the variance of one of the turbidity variables (FR-PTW_TR01QI02PV) correlates strongly (negatively) and significantly (p<0.05) with the variance in two of the Eventlab variables (FR-MOALD-vitnor1 and FR-MOALD-vitnor3)
* We got a hint of other quantities that might be related by plotting some of the correlations of their hourly variances. More research is needed for this.




## Further Research

#### Additional methods

1. **Clustering** of variables (exploratory)
    * Mostly useful to investigate which variables are closely related
    * But works best *within* datasets of similar measured quantities (such as *only* flow data or pressure data)
1. **Regression Analysis**
    * Complicated because of lack of domain knowledge (risk of including theoretically unrelated regressors, etc)
1. **Machine Learning**:
    * Specify *categorization problem* in which the Eventlab variable has 2 status levels 'normal'/'alarm' if below/above threshold (or even better: 'green'/'orange'/'red')
    * Split dataset into training and test sets
    * Apply ML algorithm (Random Forest, Boosting?)
    * Predict and validate
    * We are intested in list of most important predictors!


#### Improved subsetting and aggregation

* **Extend time period**: larger data subsets to train more accurate models
* **Different response variables**: instead of the specific one we focused on right now
* **More measured quantities**: for example include 'other' variables (more domain knowledge needed?)
* **Smaller/larger aggregation time windows**: or don't aggregate at all
* **Aggregate different statistics**: now we determined mean, variance, minimum, maximum, anyabovethreshold per aggregation time window
* **Correlate different statistics combinations**: for example look at correlation of hourly max for Eventlab with hourly min of pressure)


#### Incorporate domain knowledge

Last but not least: it's safe to say that including more domain knowledge about **measured quantities** and **how they were gathered** may make it substantially easier to focus on more effective approaches and to draw meaningful conclusions.



## Data Source

Data set provided by Vitens.