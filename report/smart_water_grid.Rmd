---
title: "Correlating water quality fluctuations with sensor data"
subtitle: "Vitens Open Data Challenge: Smart Water Grid"
author: "Dennis van den Berg"
date: "24/01/2016"
output:
  html_document:
    fig_width: 10
---


## Introduction

Central question: What are the correlations between changes in water quality measued by Eventlab sensors and other real-time measurements, statuses and alarm values?

See documentation: https://github.com/dljvandenberg/smart_water_grid/blob/master/doc/Vitens%20Data%20Challenge%2007122015.pdf


## Data Description

```{r, message=FALSE, echo=FALSE}
library(dplyr)
library(reshape2)
library(lubridate)
library(ggplot2)
library(caret)
library(GGally)     # for ggpairs
library(Rmisc)
library(gplots)     # for heatmap.2

dir.data <- "~/git/smart_water_grid/data"
dir.cache <- "~/git/smart_water_grid/cache"
setwd(dir.data)

createvariablefromfilename <- function(filename) {
    # Chop off beginning and end of filename and convert to valid variable name
    variablename <- make.names(gsub("(^Dump_|.csv$)", "", filename))
}

readfile <- function(file) {
    variablename <- createvariablefromfilename(file)
    
    data <- read.csv(file, comment.char = "#", col.names = c("Time", "Value"), colClasses = c("character", "numeric"),
                     na.strings = c("", " ", "CalcFailed", "Calc Failed", "Bad", "BadInput", "Bad Input", "PtCreated", "Pt Created", "CommFail", "ScanOff", "Configure", "I/OTimeout"))
    
    # Check for empty file
    if(nrow(data) != 0) {
        # Convert to POSIXct/POSIXt time format
        data$Time <- ymd_hms(data$Time, tz = "UTC")
        
        # Add variablename as column
        data$Variable <- variablename
    } else {
        # Add Variable column
        data <- data.frame(Time=character(), Value=numeric(), Variable=character())
    }

    # Reorder columns
    return(data[, c(1, 3, 2)])
}

readfilesraw <- function(fileslist) {
    listofdataframes <- lapply(fileslist, function(file) readfile(file))
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

# Read files and aggregate by time
readfilesandaggregate <- function(fileslist, aggregate.period="hours") {
    listofdataframes <- lapply(fileslist, function(file) aggregatebytime(readfile(file), period=aggregate.period))
    
    # Merge vertically
    merged <- Reduce(function(x, y) rbind(x, y), listofdataframes)
    
    return(merged)
}

aggregatebytime <- function(dataframe, period="hours") {
    allowedperiods <- c("none", "secs", "mins", "hours", "days", "weeks", "months", "quarters", "years")

    # Calculate new aggregate vars 'mean', 'var', 'min', 'max'
    statistics <- function(values) { c(mean=mean(values), var=var(values), min=min(values), max=max(values)) }

    # Checks
    if (nrow(na.omit(dataframe)) == 0) {
        return(na.omit(dataframe))
    }
     if (! period %in% allowedperiods) {
         warning(paste("Invalid period option ", "'", period, "'", sep=""))         
     } else {
         if (! period == "none") {
             # Break into time periods
             dataframe$Time <- as.POSIXct(cut(dataframe$Time, breaks=period))
         }
     }
    
    # Aggregate and convert to proper data frame
    newdataframe <- as.data.frame(as.list(aggregate(Value ~ Time + Variable, dataframe, FUN=statistics)))
    
    # Give proper variable name
    names(newdataframe) <- gsub("Value.", "", names(newdataframe))
    
    return(newdataframe)
}

selecttimerange <- function(dataframe, begintime = -Inf, endtime = Inf) {
    subset(dataframe, Time >= begintime & Time <= endtime)
}

wideformat <- function(df.timevariablevalue) {
    # Melt 'Statistics' column
    df.responsepredictors.molten <- melt(df.timevariablevalue, id.vars=c("Time", "Variable"), variable.name="Statistic", value.name="Value")
    
    # Cast
    dcast(df.responsepredictors.molten, Time ~ Variable + Statistic, value.var="Value")
}
```


### Data files

```{r, echo=FALSE}
files.all <- list.files(path=dir.data, pattern="csv$")
```

Our data set contains `r length(files.all)` files and its total size is a little over 5 GB. All files are in .csv format and contain Timestamp-Value pairs. Furthermore the data files contain some additional information in the header comments. 

The file name contains the variable that was measured, in format `Dump_<variable>.csv`:

```{r, echo=FALSE}
head(files.all, n=4)
```



### Time range

According to the documentation measurements are done in the time range January 2014 to December 2015. We confirmed this by looking at the data. We also observe that some variables are only available in specific time ranges, often data is missing in large blocks of this 2-year time range.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
files.select <- list.files(path=dir.data, pattern="vitnor.*csv$")[1:3]
data.select <- readfilesraw(files.select)
summary(data.select)
ggplot(data.select, aes(x=Time, y=Value, color=Variable)) + geom_point() + ggtitle("Raw data (unaggregated)")
```

The frequency of data logging is 1 measurement per minute at most, though certainly not for all variables.


### Variable overview

In the documentation we see the following explanation of variable names:

Quantity      | Variable code    | Remarks
------------- | -------------    | -------------
Eventlab data | `*vitnor*`       | 3 per sensor (`vitnor1`, `vitnor2`, `vitnor3`). Alarm codes are given in case 1 or more values above threshold (code Orange for `1 > max(vitnor*) >= 1.5`, code Red for `max(vitnor*) > 1.5`)
Temperature   | `*TM*`           | 
Flow          | `*FT*`, `*VO*`   | Negative values allowed (in case of opposite flow direction)
Pressure      | `*PT*`, `*DO*`   | We found that these had to be followed by 2 digits (`*PTxx*`, `*DOxx*`) in order not to select incorrect variables 
Conductivity  | `FR-PNB_TR00QI03PV*`, `FR-POH_-TR00QI03PV*` | 
Acidity (pH)  | `FR-PNB_TR00QI01PV*`, `FR-POH_-TR00QI02PV*`, `FR-PSP-TR00QI01*`, `FR-PTW_TR01QI01PV*` | 
Turbidity     | `FR-PTW_TR01QI02PV*`, `FR-PSP-TR00QI02*`, `FR-PNB_TR00QI02PV*`, `FR-POH_-TR00QI01PV*` | 
Other         | (None of the above) | Status values of pumps, reservoirs, valves, etc

```{r, echo=FALSE}
files.eventlab <- list.files(path=dir.data, pattern="vitnor.*csv$")
files.temperature <- list.files(path=dir.data, pattern="TM.*csv$")
files.flow <- list.files(path=dir.data, pattern="(FT|VO).*csv$")
files.pressure <- list.files(path=dir.data, pattern="(PT|DO)[0123456789][0123456789]-.*csv$")
files.conductivity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI03PV|FR-POH_-TR00QI03PV).*csv$")
files.acidity <- list.files(path=dir.data, pattern="(FR-PNB_TR00QI01PV|FR-POH_-TR00QI02PV|FR-PSP-TR00QI01|FR-PTW_TR01QI01PV).*csv$")
files.turbidity <- list.files(path=dir.data, pattern="(FR-PTW_TR01QI02PV|FR-PSP-TR00QI02|FR-PNB_TR00QI02PV|FR-POH_-TR00QI01PV).*csv$")
files.other <- Reduce(setdiff, list(files.all, files.eventlab, files.temperature, files.flow, files.pressure, files.conductivity, files.acidity, files.turbidity))
```


### Predictor and response variables

We are interested in Eventlab measurements (variable names `*vitnor*`) as **response variables**

* There are `r length(files.eventlab)` variables for Eventlab measurements
* These represent `r length(files.eventlab)/3` Eventlab sensors, each of which produce a set of three variables (`*vitnor1*`, `*vitnor2*` and `*vitnor3*`)
* Specifically, we are interested in events with values above a threshold for these variables (>1).

That leaves `r length(files.all)-length(files.eventlab)` variables that we can use as **potential predictors**:

* `r length(files.temperature)` for temperature
* `r length(files.flow)` for flow
* `r length(files.pressure)` for pressure
* `r length(files.conductivity)` for conductivity
* `r length(files.acidity)` for acidity
* `r length(files.turbidity)` for turbidity
* `r length(files.other)` other

The data quality of these variables varies. Some files do not even contain any data points.

We decided not to focus on the 'other' variable for now, as we have limited background of their meaning.


## Exploratory Analysis

### Zooming in on Eventlab data

```{r, echo=FALSE}
pattern.response="FR-MOBMS-vitnor1-meetwaarde"
time.begin.1=ymd("2015-06-27", tz = "UTC")
time.end.1=ymd("2015-07-04", tz = "UTC")
time.begin.2=ymd_hms("2015-06-29T00:00:00", tz = "UTC")
time.end.2=ymd_hms("2015-06-29T12:00:00", tz = "UTC")
time.begin.3=ymd_hms("2015-06-29T02:00:00", tz = "UTC")
time.end.3=ymd_hms("2015-06-29T03:00:00", tz = "UTC")
threshold.orange=1
threshold.red=1.5
files.response <- list.files(path=dir.data, pattern=pattern.response)
variables.response <- createvariablefromfilename(files.response)
```

Based on large blocks of consecutive data seen in exploratory plots and multiple peaks of Eventlab variables exceeding the threshold we decided to focus on the `r variables.response` variable as response variable.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
setwd(dir.data)
df.eventlab.response <- readfilesraw(files.response)
plot.response <- ggplot(df.eventlab.response, aes(x=Time, y=Value)) + geom_point() + geom_hline(yintercept=threshold.orange, color="orange") + geom_hline(yintercept=threshold.red, color="red")
plot.response + ggtitle("Eventlab - days timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.1, time.end.1)))
plot.response + ggtitle("Eventlab - hours timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.2, time.end.2)))
```
Zooming in on a specific Eventlab alarm event starting around `r time.begin.3` we observe that the vitnor values can rise and exceed the threshold (>1 resp >1.5) abruptly in a matter of minutes and that this process can repeat itself in less than an hour:
```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.response + ggtitle("Eventlab - minutes timescale") + scale_x_datetime(limits = as.POSIXct(c(time.begin.3, time.end.3)))
```



### Aggregate data per hour

We decided to aggregate hourly data (mean, var, min, max of hour blocks) in order to compress our data set, to reduce the chances of different variables both having a value for a given time, to aggregate the number of peak events (in Eventlab measurements; only 1 per hour now) and also to increase our chances of finding correlated signals in which a small time lag between variables is present (hour time scale).

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
# df.eventlab.hourly <- readfilesandaggregate(files.eventlab, aggregate.period="hours")
# df.temperature.hourly <- readfilesandaggregate(files.temperature, aggregate.period="hours")
# df.flow.hourly <- readfilesandaggregate(files.flow, aggregate.period="hours")
# df.pressure.hourly <- readfilesandaggregate(files.pressure, aggregate.period="hours")
# df.conductivity.hourly <- readfilesandaggregate(files.conductivity, aggregate.period="hours")
# df.acidity.hourly <- readfilesandaggregate(files.acidity, aggregate.period="hours")
# df.turbidity.hourly <- readfilesandaggregate(files.turbidity, aggregate.period="hours")
# # Ignoring 'other' variables for now

# # Write/read aggregated datasets to/from dir.cache
setwd(dir.cache)
# save(df.eventlab.hourly, file="eventlab.hourly.Rdata")
# save(df.temperature.hourly, file="temperature.hourly.Rdata")
# save(df.flow.hourly, file="flow.hourly.Rdata")
# save(df.pressure.hourly, file="pressure.hourly.Rdata")
# save(df.conductivity.hourly, file="conductivity.hourly.Rdata")
# save(df.acidity.hourly, file="acidity.hourly.Rdata")
# save(df.turbidity.hourly, file="turbidity.hourly.Rdata")
load(file="eventlab.hourly.Rdata")
load(file="temperature.hourly.Rdata")
load(file="flow.hourly.Rdata")
load(file="pressure.hourly.Rdata")
load(file="conductivity.hourly.Rdata")
load(file="acidity.hourly.Rdata")
load(file="turbidity.hourly.Rdata")
setwd(dir.data)
```


### Hourly maximums for Eventlab data

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.max <- ggplot(df.eventlab.hourly, aes(x=Time, y=max, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly max)") + geom_hline(yintercept=1, color="orange") + geom_hline(yintercept=1.5, color="red")
plot.eventlab.hourly.max + scale_y_log10()
```

We notice many peak events above the orange and red thresholds (>1 resp >1.5) for Eventlab sensor data.


### Hourly means for other (potential predictor) variables

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.mean <- ggplot(df.temperature.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Temperature (hourly means)")
#plot.flow.hourly.mean <- ggplot(df.flow.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Flow (hourly means)")
plot.pressure.hourly.mean <- ggplot(df.pressure.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Pressure (hourly means)")
plot.conductivity.hourly.mean <- ggplot(df.conductivity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly means)")
plot.acidity.hourly.mean <- ggplot(df.acidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Acidity (hourly means)")
plot.turbidity.hourly.mean <- ggplot(df.turbidity.hourly, aes(x=Time, y=mean, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly means)")

plot.temperature.hourly.mean
#plot.flow.hourly.mean
plot.pressure.hourly.mean + scale_y_log10()
plot.conductivity.hourly.mean + scale_y_log10()
plot.acidity.hourly.mean + scale_y_log10()
plot.turbidity.hourly.mean + scale_y_log10()
```


### Hourly variances

Because we are interested in correlated peak events, we also looked at the hourly variance of these variables, as a measure of their variability on the timescale of less than an hour.

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.eventlab.hourly.var <- ggplot(df.eventlab.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Eventlab measurements (hourly variances)")
plot.eventlab.hourly.var + scale_y_log10()
```

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
plot.temperature.hourly.var <- ggplot(df.temperature.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Temperature (hourly vars)")
#plot.flow.hourly.var <- ggplot(df.flow.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Flow (hourly vars)")
plot.pressure.hourly.var <- ggplot(df.pressure.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Pressure (hourly vars)")
plot.conductivity.hourly.var <- ggplot(df.conductivity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Conductivity (hourly vars)")
plot.acidity.hourly.var <- ggplot(df.acidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Acidity (hourly vars)")
plot.turbidity.hourly.var <- ggplot(df.turbidity.hourly, aes(x=Time, y=var, color=Variable)) + geom_point() + ggtitle("Turbidity (hourly vars)")

plot.temperature.hourly.var + scale_y_log10()
#plot.flow.hourly.var + scale_y_log10()
plot.pressure.hourly.var + scale_y_log10()
plot.conductivity.hourly.var + scale_y_log10()
plot.acidity.hourly.var + scale_y_log10()
plot.turbidity.hourly.var + scale_y_log10()
```



### Heatmaps of Eventlab alarm events

```{r, warning=FALSE, message=FALSE, cache=TRUE, echo=FALSE}
# Select subset of vitnor variables, read data for specific time range and plot heatmap
setwd(dir.data)
pattern.eventlab="FR-MOBMS-vitnor"
files.eventlab <- list.files(path=dir.data, pattern=pattern.eventlab)
data.eventlab <- readfilesraw(files.eventlab)
time.begin=ymd_hms("2015-05-20T00:00:00", tz = "UTC")
time.end=ymd_hms("2015-06-30T00:00:00", tz = "UTC")
data.eventlab.timerange <- selecttimerange(data.eventlab, time.begin, time.end)
#heatmap.abovethreshold(data.eventlab.timerange, threshold=1)
```


### Correlations



## Further Research

Data import & aggregation:

1. Aggregate above threshold TRUE/FALSE (apply to Eventlab data)


Summarizations & visualizations:

1. Multivariate plots -> focus on interesting time range
1. Pair plots
1. Heatmaps (al least for eventlab, possibly for other categories)
1. Dendrogram (related vars within categories)


Machine Learning & Predictions:

1. Select 1 response var
1. Select 1 interesting dataset with predictors (for example turbidity)
1. Select interesting time range (optional)
1. Merge predictor set and response var (and remove NA): Melt > select variance > cast > merge > omit.na

1. Split into train and test sets
1. Apply ML algorithm (RF, GBM, Tree model)
1. Predict and validate
1. Check list of important predictors



## Data Source

The set of raw data was provided by Vitens.